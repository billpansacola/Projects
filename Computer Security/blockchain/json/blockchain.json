{"length": 2, "blockchain": [{"timestamp": "Genesis Block", "transactions": [], "previousBlockHash": null, "index": 0, "proof": 100, "blockHash": "2fdcb25c77053715b9fddaa55f32d598f2237dbbcc558d6bd4bfc5c61de0f007", "module": "block", "class": "Block"}, {"timestamp": "There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers\u2019 intuition of what constitutes a \u2018good\u2019 explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.", "transactions": [], "previousBlockHash": "2fdcb25c77053715b9fddaa55f32d598f2237dbbcc558d6bd4bfc5c61de0f007", "index": 1, "proof": 52838, "blockHash": "c8c5d5b80f65fb500e23f4288796a8551422aaa5c940c29022441358b6effc30", "module": "block", "class": "Block"}, {"timestamp": "Recently, the notion of explainable artificial intelligence has seen a resurgence, after having slowed since the burst of work on explanation in expert systems over three decades ago; for example, see Chandrasekaran et al. [23], [168], and Buchanan and Shortliffe [14]. Sometimes abbreviated XAI (eXplainable artificial intelligence), the idea can be found in grant solicitations [32] and in the popular press [136]. This resurgence is driven by evidence that many AI applications have limited take up, or are not appropriated at all, due to ethical concerns [2] and a lack of trust on behalf of their users [166,101]. The running hypothesis is that by building more transparent, interpretable, or explainable systems, users will be better equipped to understand and therefore trust the intelligent agents [129,25,65]. While there are many ways to increase trust and transparency of intelligent agents, two complementary approaches will form part of many trusted autonomous systems: (1) generating decisions1 in which one of the criteria taken into account during the computation is how well a human could understand the decisions in the given context, which is often called interpretability or explainability; and (2) explicitly explaining decisions to people, which we will call explanation. Applications of explanation are considered in many sub-fields of artificial intelligence, such as justifying autonomous agent behaviour [129,65], debugging of machine learning models [89], explaining medical decision-making [45], and explaining predictions of classifiers [157].", "transactions": [], "previousBlockHash": "c8c5d5b80f65fb500e23f4288796a8551422aaa5c940c29022441358b6effc30", "index": 2, "proof": 27562, "blockHash": "996e269fcc2ab016ceb6f85bc0bc2b363f5c2630e1abd8771ded5f689de6821e", "module": "block", "class": "Block"}], "module": "chain", "class": "Chain"}